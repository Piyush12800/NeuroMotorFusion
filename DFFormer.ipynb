{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a486cde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from easydict import EasyDict\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a35418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_sincos_pos_embed(dim: int, seq_len: int, cls_token: bool = False):\n",
    "    if cls_token:\n",
    "        pe = torch.zeros(seq_len + 1, dim)\n",
    "        position = torch.arange(0, seq_len + 1, dtype=torch.float).unsqueeze(1)\n",
    "    else:\n",
    "        pe = torch.zeros(seq_len, dim)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0)\n",
    "\n",
    "    return pe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec70bb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_output_size(input_size, layers):\n",
    "    output_size = input_size\n",
    "    for layer in layers:\n",
    "        kernel_size, stride = layer[2], layer[3]\n",
    "        output_size = math.floor((output_size - kernel_size) / stride) + 1\n",
    "    return output_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe3b0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# def calculate_output_size(input_size, layers):\n",
    "#     output_size = input_size\n",
    "#     for layer in layers:\n",
    "#         kernel_size, stride, padding = layer[2], layer[3], layer[4]\n",
    "#         output_size = math.floor((output_size + 2 * padding - kernel_size) / stride) + 1\n",
    "#     return output_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca02aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features,\n",
    "        hidden_features=None,\n",
    "        out_features=None,\n",
    "        act_layer=nn.GELU,\n",
    "        drop=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45f3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.attn_gradients = None\n",
    "        self.attention_map = None\n",
    "        self.value_gradients = None\n",
    "\n",
    "    def save_attn_gradients(self, attn_gradients):\n",
    "        self.attn_gradients = attn_gradients\n",
    "\n",
    "    def get_attn_gradients(self):\n",
    "        return self.attn_gradients\n",
    "\n",
    "    def save_attention_map(self, attention_map):\n",
    "        self.attention_map = attention_map\n",
    "\n",
    "    def get_attention_map(self):\n",
    "        return self.attention_map\n",
    "\n",
    "    def save_value_gradients(self, value_gradients):\n",
    "        self.value_gradients = value_gradients\n",
    "\n",
    "    def get_value_gradients(self):\n",
    "        return self.value_gradients\n",
    "\n",
    "    def forward(self, x, register_hook=False):\n",
    "        b, n, _, h = *x.shape, self.num_heads\n",
    "\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = rearrange(qkv, \"b n (qkv h d) -> qkv b h n d\", qkv=3, h=h)\n",
    "\n",
    "        dots = torch.einsum(\"bhid,bhjd->bhij\", q, k) * self.scale\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = torch.einsum(\"bhij,bhjd->bhid\", attn, v)\n",
    "\n",
    "        self.save_attention_map(attn)\n",
    "        if register_hook:\n",
    "            v.register_hook(self.save_value_gradients)\n",
    "            attn.register_hook(self.save_attn_gradients)\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a524cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransposeLast(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.transpose(-2, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6025b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers: List[Tuple[int, int, int, int]],\n",
    "        bias: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(\n",
    "                        in_dim,\n",
    "                        out_dim,\n",
    "                        kernel_size=kernel,\n",
    "                        stride=stride,\n",
    "#                         Changes\n",
    "#                         padding= padding,\n",
    "                        bias=bias,\n",
    "                    ),\n",
    "                    TransposeLast(),\n",
    "                    nn.LayerNorm(out_dim),\n",
    "                    TransposeLast(),\n",
    "                    nn.GELU(),\n",
    "                )\n",
    "                for (in_dim, out_dim, kernel, stride) in layers\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07aac7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=False,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(\n",
    "            in_features=dim,\n",
    "            hidden_features=mlp_hidden_dim,\n",
    "            act_layer=act_layer,\n",
    "            drop=drop,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, register_hook=False):\n",
    "        x = x + self.attn(self.norm1(x), register_hook=register_hook)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf666310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalSpatialEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim: int, nhead: int, dropout_rate: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.temporal_block = Block(\n",
    "            dim=self.embed_dim,\n",
    "            num_heads=nhead,\n",
    "            mlp_ratio=1.0,\n",
    "            qkv_bias=False,\n",
    "            drop=dropout_rate,\n",
    "            attn_drop=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.spatial_block = Block(\n",
    "            dim=self.embed_dim,\n",
    "            num_heads=nhead,\n",
    "            mlp_ratio=1.0,\n",
    "            qkv_bias=False,\n",
    "            drop=dropout_rate,\n",
    "            attn_drop=dropout_rate,\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x, register_hook=False):\n",
    "        B, C, D, T = x.shape\n",
    "\n",
    "        # Temporal Block\n",
    "        x = x.reshape(B * C, D, T)  # BC x D x T\n",
    "        x = x.transpose(1, 2)  # BC x T x D\n",
    "        x = self.temporal_block(x, register_hook=register_hook)\n",
    "        x = x.reshape(B, C, T, D)  # B x C x T x D\n",
    "        x = x.transpose(1, 2)  # B x T x C x D\n",
    "\n",
    "        # Spatial Block\n",
    "        x = x.reshape(B * T, C, D)  # BT x C x D\n",
    "        x = self.spatial_block(x, register_hook=register_hook)\n",
    "        x = x.reshape(B, T, C, D)  # B x T x C x D\n",
    "        x = x.permute(0, 2, 3, 1)  # B x C x D x T\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815b505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        nhead,\n",
    "        spatial_len,\n",
    "        input_size,\n",
    "        cnn_layers,\n",
    "        dropout_rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_len = calculate_output_size(input_size, cnn_layers)\n",
    "        self.spatial_len = spatial_len\n",
    "\n",
    "        self.patch_embed = PatchEmbed(cnn_layers)\n",
    "\n",
    "        self.temporal_block = Block(\n",
    "            dim=self.embed_dim,\n",
    "            num_heads=nhead,\n",
    "            mlp_ratio=1.0,\n",
    "            qkv_bias=False,\n",
    "            drop=dropout_rate,\n",
    "            attn_drop=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.spatial_block = Block(\n",
    "            dim=self.embed_dim,\n",
    "            num_heads=nhead,\n",
    "            mlp_ratio=1.0,\n",
    "            qkv_bias=False,\n",
    "            drop=dropout_rate,\n",
    "            attn_drop=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.temporal_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, self.seq_len + 1, embed_dim), requires_grad=False\n",
    "        )\n",
    "        self.spatial_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, self.spatial_len + 1, self.embed_dim),\n",
    "            requires_grad=False,\n",
    "        )\n",
    "\n",
    "        self.temporal_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        self.spatial_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "\n",
    "        self.initialize_weights()\n",
    "        \n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        temporal_pos_embed = get_sincos_pos_embed(\n",
    "            dim=self.embed_dim, seq_len=self.seq_len, cls_token=True\n",
    "        )\n",
    "        self.temporal_pos_embed.data.copy_(temporal_pos_embed)\n",
    "\n",
    "        spatial_pos_embed = get_sincos_pos_embed(\n",
    "            dim=self.embed_dim,\n",
    "            seq_len=self.spatial_len,\n",
    "            cls_token=True,\n",
    "        )\n",
    "        self.spatial_pos_embed.data.copy_(spatial_pos_embed)\n",
    "\n",
    "        torch.nn.init.normal_(self.temporal_token, std=0.02)\n",
    "        torch.nn.init.normal_(self.spatial_token, std=0.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x, register_hook=False):\n",
    "        B, C, D, T = x.shape\n",
    "\n",
    "        # Path embedding\n",
    "        x = x.reshape(B * C, D, T)  # BC x D x T\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Temporal position embedding & block\n",
    "        x = x.transpose(1, 2)  # BC x T x D\n",
    "        x = x + self.temporal_pos_embed[:, 1:, :]\n",
    "        token = self.temporal_token + self.temporal_pos_embed[:, :1, :]\n",
    "        token = token.expand(B * C, -1, -1)\n",
    "        x = torch.cat((token, x), dim=1)\n",
    "        x = self.temporal_block(x, register_hook=register_hook)\n",
    "        x = x.reshape(B, C, -1, self.embed_dim)  # B x C x T x D\n",
    "        x = x.transpose(1, 2)  # B x T x C x D\n",
    "\n",
    "        # Spatial position embedding & block\n",
    "        B, T, C, D = x.shape\n",
    "        x = x.reshape(B * T, C, D)  # BT x C x D\n",
    "        x = x + self.spatial_pos_embed[:, 1:, :]\n",
    "        token = self.spatial_token + self.spatial_pos_embed[:, :1, :]\n",
    "        token = token.expand(B * T, -1, -1)\n",
    "        x = torch.cat((token, x), dim=1)\n",
    "        x = self.spatial_block(x, register_hook=register_hook)\n",
    "        x = x.reshape(B, -1, C + 1, self.embed_dim)  # B x T x C x D\n",
    "        x = x.permute(0, 2, 3, 1)  # B x C x D x T\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08263fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        nhead,\n",
    "        spatial_len,\n",
    "        input_size,\n",
    "        cnn_layers,\n",
    "        dropout_rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.seq_len = calculate_output_size(input_size, cnn_layers)\n",
    "        self.spatial_len = spatial_len\n",
    "\n",
    "        self.patch_embed = PatchEmbed(cnn_layers)\n",
    "\n",
    "        self.temporal_block = Block(\n",
    "            dim=self.embed_dim,\n",
    "            num_heads=nhead,\n",
    "            mlp_ratio=1.0,\n",
    "            qkv_bias=False,\n",
    "            drop=dropout_rate,\n",
    "            attn_drop=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.spatial_block = Block(\n",
    "            dim=self.embed_dim,\n",
    "            num_heads=nhead,\n",
    "            mlp_ratio=1.0,\n",
    "            qkv_bias=False,\n",
    "            drop=dropout_rate,\n",
    "            attn_drop=dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.temporal_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, self.seq_len + 1, embed_dim), requires_grad=False\n",
    "        )\n",
    "        self.spatial_pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, self.spatial_len + 1, embed_dim),\n",
    "            requires_grad=False,\n",
    "        )\n",
    "\n",
    "        self.temporal_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        self.spatial_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "\n",
    "        self.initialize_weights()\n",
    "        \n",
    "    \n",
    "    def initialize_weights(self):\n",
    "        temporal_pos_embed = get_sincos_pos_embed(\n",
    "            dim=self.embed_dim, seq_len=self.seq_len, cls_token=True\n",
    "        )\n",
    "        self.temporal_pos_embed.data.copy_(temporal_pos_embed)\n",
    "\n",
    "        spatial_pos_embed = get_sincos_pos_embed(\n",
    "            dim=self.embed_dim,\n",
    "            seq_len=self.spatial_len,\n",
    "            cls_token=True,\n",
    "        )\n",
    "        self.spatial_pos_embed.data.copy_(spatial_pos_embed)\n",
    "\n",
    "        torch.nn.init.normal_(self.temporal_token, std=0.02)\n",
    "        torch.nn.init.normal_(self.spatial_token, std=0.02)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def forward(self, x, register_hook=False):\n",
    "        B, C, D, T = x.shape\n",
    "\n",
    "        # Path embedding\n",
    "        x = x.reshape(B * C, D, T)  # BC x D x T\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        # Temporal position embedding & block\n",
    "        x = x.transpose(1, 2)  # BC x T x D\n",
    "        x = x + self.temporal_pos_embed[:, :T, :]\n",
    "        token = self.temporal_token.expand(B * C, -1, -1)\n",
    "        x = torch.cat((token, x), dim=1)\n",
    "        x = self.temporal_block(x, register_hook=register_hook)\n",
    "        x = x.reshape(B, C, -1, self.embed_dim)  # B x C x T x D\n",
    "        x = x.transpose(1, 2)  # B x T x C x D\n",
    "\n",
    "        # Spatial position embedding & block\n",
    "        B, T, C, D = x.shape\n",
    "        x = x.reshape(B * T, C, D)  # BT x C x D\n",
    "        x = x + self.spatial_pos_embed[:, :D, :]\n",
    "        token = self.spatial_token.expand(B * T, -1, -1)\n",
    "        x = torch.cat((token, x), dim=1)\n",
    "        x = self.spatial_block(x, register_hook=register_hook)\n",
    "        x = x.reshape(B, -1, C + 1, self.embed_dim)  # B x T x C x D\n",
    "        x = x.permute(0, 2, 3, 1)  # B x C x D x T\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b9b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        num_classes,\n",
    "        num_channels,\n",
    "        seq_len,\n",
    "        use_token,\n",
    "        dropout_rate: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.use_token = use_token\n",
    "\n",
    "        if self.use_token:\n",
    "            self.mlp_head = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear((num_channels + 1 + seq_len) * embed_dim, num_classes),\n",
    "            )\n",
    "        else:\n",
    "            self.mlp_head = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    embed_dim,\n",
    "                    embed_dim,\n",
    "                    kernel_size=(num_channels, 1),\n",
    "                    bias=False,\n",
    "                ),\n",
    "                nn.ELU(),\n",
    "                nn.BatchNorm2d(embed_dim),\n",
    "                nn.Dropout2d(dropout_rate),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(seq_len * embed_dim, num_classes),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.use_token:\n",
    "            B, C, D, T = x.shape\n",
    "            x = x.transpose(1, 2)  # B x D x C x T\n",
    "        else:\n",
    "            B, C, D = x.shape\n",
    "            x = x.reshape(B * C, self.embed_dim)\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e808c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=128,\n",
    "        nhead=8,\n",
    "        inter_information_length=22,\n",
    "        origin_ival=(1, 64, 3, 1),\n",
    "        cnn_layers=[(1, 64, 3, 1), (64, 128, 3, 1)],\n",
    "        nlayer=4,\n",
    "        num_classes=2,\n",
    "        use_token=True,\n",
    "        apply_cls_head=True,\n",
    "#         db_name=\"BCIC2a\",\n",
    "        dropout_rate=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.use_token = use_token\n",
    "        self.apply_cls_head = apply_cls_head\n",
    "\n",
    "        self.embedding = Embedding(\n",
    "            self.embed_dim,\n",
    "            nhead,\n",
    "            inter_information_length,\n",
    "            origin_ival[-1],\n",
    "            cnn_layers,\n",
    "            dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TemporalSpatialEncoder(self.embed_dim, nhead, dropout_rate)\n",
    "                for _ in range(nlayer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if self.apply_cls_head:\n",
    "            self.classifier_head = ClassifierHead(\n",
    "                self.embed_dim,\n",
    "                num_classes,\n",
    "                inter_information_length,\n",
    "                self.embedding.seq_len,\n",
    "#                 db_name,\n",
    "                use_token,\n",
    "                dropout_rate,\n",
    "            )\n",
    "\n",
    "    def forward(self, x, register_hook=False):\n",
    "        x = self.embedding(x, register_hook=register_hook)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = x + block(x, register_hook=register_hook)  # B x C x D x T\n",
    "\n",
    "        if self.apply_cls_head:\n",
    "            x = self.classifier_head(x[:, 1:, :, 1:])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f23f6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim=128,\n",
    "        nhead=8,\n",
    "        inter_information_length=22,\n",
    "        origin_ival=(1, 64, 3, 1),\n",
    "        cnn_layers=[(1, 64, 3, 1), (64, 128, 3, 1)],\n",
    "        nlayer=4,\n",
    "        num_classes=2,\n",
    "        use_token=True,\n",
    "        apply_cls_head=True,\n",
    "        dropout_rate=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.use_token = use_token\n",
    "        self.apply_cls_head = apply_cls_head\n",
    "\n",
    "        self.embedding = Embedding(\n",
    "            self.embed_dim,\n",
    "            nhead,\n",
    "            inter_information_length,\n",
    "            origin_ival[-1],\n",
    "            cnn_layers,\n",
    "            dropout_rate,\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                TemporalSpatialEncoder(self.embed_dim, nhead, dropout_rate)\n",
    "                for _ in range(nlayer)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        if self.apply_cls_head:\n",
    "            self.classifier_head = ClassifierHead(\n",
    "                self.embed_dim,\n",
    "                num_classes,\n",
    "                inter_information_length,\n",
    "                self.embedding.seq_len,\n",
    "                use_token,\n",
    "                dropout_rate,\n",
    "            )\n",
    "\n",
    "    def forward(self, x, register_hook=False):\n",
    "        x = self.embedding(x, register_hook=register_hook)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = x + block(x, register_hook=register_hook)\n",
    "\n",
    "        if self.apply_cls_head:\n",
    "            x = self.classifier_head(x[:, 1:, :, 1:])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c34497bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to create tensor with negative dimension -2: [1, -2, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDFformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "Cell \u001b[1;32mIn [18], line 22\u001b[0m, in \u001b[0;36mDFformer.__init__\u001b[1;34m(self, embed_dim, nhead, inter_information_length, origin_ival, cnn_layers, nlayer, num_classes, use_token, apply_cls_head, dropout_rate)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_token \u001b[38;5;241m=\u001b[39m use_token\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_cls_head \u001b[38;5;241m=\u001b[39m apply_cls_head\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m \u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnhead\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43minter_information_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43morigin_ival\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcnn_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m     32\u001b[0m     [\n\u001b[0;32m     33\u001b[0m         TemporalSpatialEncoder(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, nhead, dropout_rate)\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nlayer)\n\u001b[0;32m     35\u001b[0m     ]\n\u001b[0;32m     36\u001b[0m )\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_cls_head:\n",
      "Cell \u001b[1;32mIn [10], line 37\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[1;34m(self, embed_dim, nhead, spatial_len, input_size, cnn_layers, dropout_rate)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_block \u001b[38;5;241m=\u001b[39m Block(\n\u001b[0;32m     19\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[0;32m     20\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mnhead,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     attn_drop\u001b[38;5;241m=\u001b[39mdropout_rate,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_block \u001b[38;5;241m=\u001b[39m Block(\n\u001b[0;32m     28\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim,\n\u001b[0;32m     29\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mnhead,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     attn_drop\u001b[38;5;241m=\u001b[39mdropout_rate,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_pos_embed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\n\u001b[1;32m---> 37\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m)\u001b[49m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     38\u001b[0m )\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_pos_embed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(\n\u001b[0;32m     40\u001b[0m     torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim),\n\u001b[0;32m     41\u001b[0m     requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_token \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -2: [1, -2, 128]"
     ]
    }
   ],
   "source": [
    "model = DFformer()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29899759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "796e0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_BCI2a_data_directory(data_dir, training, all_trials=True):\n",
    "    \"\"\" Loading and Dividing of the data set based on the subject-specific \n",
    "    (subject-dependent) approach.\n",
    "    In this approach, we used the same training and testing data as the original\n",
    "    competition, i.e., 288 x 9 trials in session 1 for training, \n",
    "    and 288 x 9 trials in session 2 for testing.  \n",
    "   \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir: string\n",
    "        Directory containing dataset files\n",
    "    training: bool\n",
    "        If True, load training data\n",
    "        If False, load testing data\n",
    "    all_trials: bool, optional\n",
    "        If True, load all trials\n",
    "        If False, ignore trials with artifacts \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data: numpy array\n",
    "        Loaded data\n",
    "    labels: numpy array\n",
    "        Loaded labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Define MI-trials parameters\n",
    "    n_channels = 22\n",
    "    n_tests = 6 * 48\n",
    "    window_Length = 7 * 250\n",
    "\n",
    "    # Define MI trial window\n",
    "    fs = 250          # sampling rate\n",
    "    t1 = int(1.5 * fs)  # start time_point\n",
    "    t2 = int(6 * fs)    # end time_point\n",
    "\n",
    "    class_return = []\n",
    "    data_return = []\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith(\".mat\"):\n",
    "            filepath = os.path.join(data_dir, filename)\n",
    "            a = sio.loadmat(filepath)\n",
    "            a_data = a['data']\n",
    "            for ii in range(0, a_data.size):\n",
    "                a_data1 = a_data[0, ii]\n",
    "                a_data2 = [a_data1[0, 0]]\n",
    "                a_data3 = a_data2[0]\n",
    "                a_X = a_data3[0]\n",
    "                a_trial = a_data3[1]\n",
    "                a_y = a_data3[2]\n",
    "                a_artifacts = a_data3[5]\n",
    "\n",
    "                for trial in range(0, a_trial.size):\n",
    "                    if a_artifacts[trial] != 0 and not all_trials:\n",
    "                        continue\n",
    "                    data = np.transpose(a_X[int(a_trial[trial]):(int(a_trial[trial]) + window_Length), :22])\n",
    "                    data_return.append(data[:, t1:t2])  # Adjusted indexing here\n",
    "                    class_return.append(int(a_y[trial]) - 1)\n",
    "\n",
    "    data_return = np.array(data_return)\n",
    "    class_return = np.array(class_return)\n",
    "\n",
    "    return data_return, class_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import scipy.io as sio\n",
    "\n",
    "# def load_BCI2a_data_directory(data_dir, training, all_trials=True):\n",
    "#     n_channels = 22\n",
    "#     window_length = 7 * 250  # 7 seconds * 250 samples/second\n",
    "#     t1 = int(1.5 * 250)  # Start time point\n",
    "#     t2 = int(6 * 250)    # End time point\n",
    "\n",
    "#     data_return = []\n",
    "#     class_return = []\n",
    "\n",
    "#     for filename in os.listdir(data_dir):\n",
    "#         if filename.endswith(\".mat\"):\n",
    "#             filepath = os.path.join(data_dir, filename)\n",
    "#             mat_data = sio.loadmat(filepath)\n",
    "#             data = mat_data['data'][0, 0][0]\n",
    "#             trial_info = mat_data['data'][0, 0][1]\n",
    "#             labels = mat_data['data'][0, 0][2]\n",
    "#             artifacts = mat_data['data'][0, 0][5]\n",
    "\n",
    "#             for i in range(len(trial_info)):\n",
    "#                 if artifacts[i] != 0 and not all_trials:\n",
    "#                     continue\n",
    "#                 trial_data = np.transpose(data[int(trial_info[i]):int(trial_info[i]) + window_length, :n_channels])\n",
    "#                 trial_data = trial_data[:, t1:t2]  # Adjusted indexing\n",
    "#                 data_return.append(trial_data)\n",
    "#                 class_return.append(int(labels[i]) - 1)\n",
    "\n",
    "#     data_return = np.array(data_return, dtype=np.float32)\n",
    "#     class_return = np.array(class_return, dtype=np.int64)\n",
    "\n",
    "#     return data_return, class_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596b6535",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'BCICIV_2a_mat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d3fea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "data , label = load_BCI2a_data_directory(data_dir, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6810fdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  2.63671875,  -4.00390625, -11.9140625 , ...,  -0.09765625,\n",
       "          -1.953125  ,  -6.8359375 ],\n",
       "        [  2.734375  ,  -3.515625  ,  -7.32421875, ...,   1.46484375,\n",
       "           0.68359375,  -4.54101562],\n",
       "        [  4.54101562,  -4.58984375, -11.9140625 , ...,   3.125     ,\n",
       "           1.80664062,  -5.22460938],\n",
       "        ...,\n",
       "        [  1.85546875,  -8.05664062, -10.83984375, ...,  -0.1953125 ,\n",
       "           1.41601562,  -2.05078125],\n",
       "        [  0.53710938,  -8.15429688, -10.9375    , ...,  -0.390625  ,\n",
       "           0.73242188,  -3.75976562],\n",
       "        [  1.51367188,  -9.1796875 , -10.64453125, ...,  -0.04882812,\n",
       "           2.49023438,  -0.92773438]],\n",
       "\n",
       "       [[ -0.29296875,   0.29296875,  -0.83007812, ...,  -9.08203125,\n",
       "          -9.66796875,  -8.15429688],\n",
       "        [  0.390625  ,   1.85546875,  -1.31835938, ...,  -4.54101562,\n",
       "          -7.91015625,  -7.08007812],\n",
       "        [ -2.34375   ,  -2.24609375,  -3.85742188, ...,  -9.22851562,\n",
       "         -10.44921875,  -9.27734375],\n",
       "        ...,\n",
       "        [ -2.63671875,  -4.8828125 ,  -5.95703125, ...,  -1.61132812,\n",
       "          -2.88085938,  -3.7109375 ],\n",
       "        [ -3.125     ,  -4.73632812,  -4.73632812, ...,   2.24609375,\n",
       "           0.        ,  -2.00195312],\n",
       "        [ -2.44140625,  -6.39648438,  -5.81054688, ...,   6.0546875 ,\n",
       "           4.58984375,   3.41796875]],\n",
       "\n",
       "       [[  7.12890625,   8.15429688,   6.4453125 , ...,   5.078125  ,\n",
       "           6.15234375,   2.58789062],\n",
       "        [ 10.7421875 ,  10.30273438,   9.08203125, ...,   4.1015625 ,\n",
       "           1.85546875,  -1.953125  ],\n",
       "        [  8.15429688,   8.984375  ,   9.22851562, ...,   1.3671875 ,\n",
       "           2.5390625 ,  -1.41601562],\n",
       "        ...,\n",
       "        [  6.78710938,   7.76367188,   7.86132812, ...,   1.66015625,\n",
       "           3.22265625,  -1.171875  ],\n",
       "        [  6.78710938,   7.51953125,   8.49609375, ...,   3.3203125 ,\n",
       "           4.19921875,  -0.09765625],\n",
       "        [  7.6171875 ,   7.12890625,   6.640625  , ...,   3.7109375 ,\n",
       "           4.19921875,  -0.34179688]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  7.12890625,  17.23632812,  13.23242188, ...,  -3.95507812,\n",
       "          -0.9765625 ,  -5.6640625 ],\n",
       "        [  9.765625  ,  16.6015625 ,  15.38085938, ...,   3.41796875,\n",
       "           2.34375   ,  -0.390625  ],\n",
       "        [  7.71484375,  18.26171875,  16.11328125, ...,   4.24804688,\n",
       "           9.42382812,   4.00390625],\n",
       "        ...,\n",
       "        [  8.69140625,  21.77734375,  25.24414062, ...,  26.5625    ,\n",
       "          30.51757812,  29.4921875 ],\n",
       "        [  0.5859375 ,  14.2578125 ,  18.50585938, ...,  31.10351562,\n",
       "          33.93554688,  31.15234375],\n",
       "        [  5.2734375 ,  15.4296875 ,  17.43164062, ...,  23.33984375,\n",
       "          27.58789062,  27.29492188]],\n",
       "\n",
       "       [[  5.2734375 ,   6.640625  ,   7.71484375, ..., -18.31054688,\n",
       "         -16.30859375, -17.52929688],\n",
       "        [  0.63476562,   4.34570312,   3.125     , ..., -30.41992188,\n",
       "         -27.1484375 , -25.390625  ],\n",
       "        [  1.3671875 ,   2.68554688,   2.734375  , ..., -27.00195312,\n",
       "         -27.09960938, -27.19726562],\n",
       "        ...,\n",
       "        [-13.52539062,  -6.73828125,  -3.66210938, ...,   0.78125   ,\n",
       "           1.12304688,  -0.24414062],\n",
       "        [-12.109375  ,  -5.81054688,  -3.07617188, ...,   3.41796875,\n",
       "           3.07617188,   0.04882812],\n",
       "        [-17.62695312,  -8.69140625,  -2.5390625 , ...,   4.58984375,\n",
       "           5.22460938,   3.7109375 ]],\n",
       "\n",
       "       [[ 18.65234375,  20.41015625,  25.48828125, ...,   7.421875  ,\n",
       "           4.83398438,   9.5703125 ],\n",
       "        [ 15.28320312,  20.80078125,  25.87890625, ...,  10.69335938,\n",
       "           7.71484375,   8.203125  ],\n",
       "        [ 12.3046875 ,  15.33203125,  21.97265625, ...,  10.69335938,\n",
       "           5.81054688,   7.27539062],\n",
       "        ...,\n",
       "        [  8.93554688,  14.40429688,  21.63085938, ...,  -3.75976562,\n",
       "          -4.73632812,  -1.12304688],\n",
       "        [ 16.9921875 ,  22.4609375 ,  28.56445312, ...,  -2.00195312,\n",
       "          -2.19726562,   1.5625    ],\n",
       "        [  5.95703125,  12.3046875 ,  19.7265625 , ...,  -7.91015625,\n",
       "          -8.05664062,  -3.80859375]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99ef4d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4896, 22, 1125)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91461f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4896,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "525bc57d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b172c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60e05a56",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.44 GiB is allocated by PyTorch, and 368.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m batch_data \u001b[38;5;241m=\u001b[39m batch_data\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (batch_size, 1, 22, 1125)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Forward pass through the DFformer model\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, batch_labels)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [12], line 50\u001b[0m, in \u001b[0;36mDFformer.forward\u001b[1;34m(self, x, register_hook)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, register_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregister_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregister_hook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m     53\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m block(x, register_hook\u001b[38;5;241m=\u001b[39mregister_hook)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [10], line 90\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, x, register_hook)\u001b[0m\n\u001b[0;32m     88\u001b[0m token \u001b[38;5;241m=\u001b[39m token\u001b[38;5;241m.\u001b[39mexpand(B \u001b[38;5;241m*\u001b[39m C, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     89\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((token, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregister_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregister_hook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(B, C, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim)  \u001b[38;5;66;03m# B x C x T x D\u001b[39;00m\n\u001b[0;32m     92\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# B x T x C x D\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [8], line 34\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x, register_hook)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, register_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), register_hook\u001b[38;5;241m=\u001b[39mregister_hook)\n\u001b[1;32m---> 34\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [4], line 20\u001b[0m, in \u001b[0;36mMlp.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[1;32m---> 20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:682\u001b[0m, in \u001b[0;36mGELU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 6.44 GiB is allocated by PyTorch, and 368.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader  # Import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "class BCI2aDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_data = torch.from_numpy(self.data[idx]).float()  # Convert to float tensor\n",
    "        sample_label = torch.tensor(self.labels[idx])  # Convert to tensor\n",
    "\n",
    "        # Optionally, you can add data preprocessing or augmentation here\n",
    "\n",
    "        return sample_data, sample_label\n",
    "\n",
    "# Load your data\n",
    "# # Assuming you have your data and labels loaded as NumPy arrays\n",
    "# data = np.random.rand(4896, 22, 1125)  # Replace with your actual data\n",
    "# labels = np.random.randint(0, 2, size=4896)  # Replace with your actual labels\n",
    "\n",
    "# Create the dataset\n",
    "dataset = BCI2aDataset(data, label)\n",
    "\n",
    "# Create the dataloader\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define your loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "cnn_layers = [(22, 64, 3, 1, 1), (64, 128, 3, 1, 1)] \n",
    "\n",
    "# Initialize the DFformer model\n",
    "model = DFformer(\n",
    "    embed_dim=128,\n",
    "    nhead=8,\n",
    "    inter_information_length=22,\n",
    "    origin_ival=(1, 22, 1125, 1),\n",
    "#     cnn_layers=[(1, 64, 3, 1), (64, 128, 3, 1)],\n",
    "    cnn_layers=cnn_layers,\n",
    "    nlayer=4,\n",
    "    num_classes=2,\n",
    "    use_token=True,\n",
    "    apply_cls_head=True,\n",
    "    dropout_rate=0.1,\n",
    ").to(device)\n",
    "# Define your optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate as needed\n",
    "num_epochs=10\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_data, batch_labels in dataloader:\n",
    "        # Add batch dimension to batch_data\n",
    "        batch_data = batch_data.unsqueeze(1).to(device) # (batch_size, 1, 22, 1125)\n",
    "\n",
    "        # Forward pass through the DFformer model\n",
    "        output = model(batch_data).to(device)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, batch_labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch + 1} loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b4d2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3c15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da57b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009ab05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c67959c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc603f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
